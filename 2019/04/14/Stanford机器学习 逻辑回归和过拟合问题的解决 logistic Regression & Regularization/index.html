<!--
	作者：Sariay
	时间：2018-09-25
	描述：There may be a bug, but don't worry, QiLing(器灵) says that it can work normally!
-->


	<!DOCTYPE html>
	<html>
		

<head><meta name="generator" content="Hexo 3.8.0">
	<title>Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression &amp; Regularization</title>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="apple-mobile-web-app-title" content="Amaze UI">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
    <meta name="author" content="miraitowa">
    <meta name="keywords" content="">
    <meta name="description" content="">
   	<!-- css -->
	<link rel="stylesheet" href="/css/style.css">

	<!-- favicon -->
	<link href="/img/favicon.ico" rel="Shortcut Icon" type="image/ico">
	
	<!-- font-awesome -->
	<link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
</head>

	<body>	
		<!--Preloader-->
<div id="preloader">
	<div id="status">
		<img alt="PRELOADER" src="/img/logo.png">
	</div>
</div>
<!--Preloader end-->

<!-- header -->

	<header id="header-bg-2">

	
		<div id="cd-logo"><a href="/"><img src="/img/logo.png" alt="Logo"></a></div>
	
	
	<!-- motto or description -->
		
 		<p class="motto"></p>
	
	
	<!-- current page name or title -->
	
		
		
			
			<p class="page-name">当前文章&nbsp;:&nbsp;《Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression &amp; Regularization》</p>
			
		
	
	
	<!-- others: such as change-bg, time... -->
	<p class="page-name-other">
		4/29/2019 
		<style type="text/css">
	header:after {
		content: '';
		position: relative;
		top: 0;
		left: 0;
		height: 100%;
		width: 100%;
		background: #222222;
		opacity: .5;
		z-index: -1;
	}
	
	.change-header-bg{
		font-style: normal;
	}
	.change-header-bg i{
		text-align: center;
		cursor: pointer;
		pointer-events: bounding-box;
	}
	@media(max-width:512px) {
		.change-header-bg {
			display: none;
			visibility: hidden;
		}
	}
	
</style>

<script type="text/javascript">
	function changeHeaderBg(){
		var random_bg = Math.floor(Math.random() * 109 + 1);
		var bg = 'url(https://miraitowa.github.io/Random-img/' + random_bg + '.jpg)';
		$("#header-bg-2").css("background-image", bg);
	}
</script>

<span class="change-header-bg">
	——&nbsp;<i class="fa fa-camera-retro" onclick="changeHeaderBg()"></i>	
</span>
	</p>		
</header>

<!-- nav -->
<div id="cd-nav">
	<a href="#0" title="menu" class="cd-nav-trigger"><span></span></a>

	<nav id="cd-main-nav">
		<ul>
			
      		<li class="fa fa-/">
           		<a href="/" title="主页">主页</a>	
      		</li>
    		
      		<li class="fa fa-/archives">
           		<a href="/archives" title="归档">归档</a>	
      		</li>
    		
      		<li class="fa fa-/categories">
           		<a href="/categories" title="分类">分类</a>	
      		</li>
    		
      		<li class="fa fa-/gallery">
           		<a href="/gallery" title="相册">相册</a>	
      		</li>
    		
      		<li class="fa fa-/about">
           		<a href="/about" title="关于">关于</a>	
      		</li>
    		
      		<li class="fa fa-/tags">
           		<a href="/tags" title="友链">友链</a>	
      		</li>
    		
    		
        	
		</ul>
	</nav>
</div>

		<!--main-->
		<main> 
		<div class="page-container">
		<!-- content srart -->
<div class="am-g am-g-fixed blog-fixed blog-content">
	<div class="am-u-md-8 am-u-sm-12">

		<article class="am-article blog-article-p">

			<div class="am-article-hd">
				


				<h1 class="am-article-title blog-text-center">
					
					
	
		<a href="/2019/04/14/Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression & Regularization/" itemprop="url">		
			Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression &amp; Regularization		
		</a>
	

				</h1>

				<p class="am-article-meta blog-text-center">
					<span>
						<i class="fa fa-clock-o"></i> 
						<a href="/2019/04/14/Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression & Regularization/" itemprop="url">
	<time datetime="2019-04-14T06:46:32.856Z" itemprop="datePublished">
  		2019-04-14
  </time>
</a>    
&nbsp;
					</span>
					
					<span>						
						
					</span>
				</p>
			</div>

			<div class="am-article-bd">
				<div class="content" id="post-content">
					
						<h1 id="Stanford机器学习-逻辑回归和过拟合问题的解决-logistic-Regression-amp-Regularization"><a href="#Stanford机器学习-逻辑回归和过拟合问题的解决-logistic-Regression-amp-Regularization" class="headerlink" title="Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression &amp; Regularization"></a>Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression &amp; Regularization</h1><p>作为一个大二的垃圾(QAQ)，看了两遍关于逻辑回归的视频教程，并且查阅资料，才慢慢明白其中的原理。</p>
<h2 id="第一部分：Logistic-Regression"><a href="#第一部分：Logistic-Regression" class="headerlink" title="第一部分：Logistic Regression"></a>第一部分：Logistic Regression</h2><h3 id="（一）-（二）、Classification-Hypothesis-Representation"><a href="#（一）-（二）、Classification-Hypothesis-Representation" class="headerlink" title="（一）~（二）、Classification / Hypothesis Representation"></a>（一）~（二）、Classification / Hypothesis Representation</h3><p>假设随Tumor Size变化，预测病人的肿瘤是恶性（malignant）还是良性（benign）的情况。</p>
<p>给出8个数据如下：</p>
<p><img src="https://i.imgur.com/Lg5xUvP.jpg" alt=""></p>
<p>假设进行linear regression得到的hypothesis线性方程如上图中粉线所示，则可以确定一个threshold:0.5进行predict</p>
<blockquote>
<p>y=1, if h(x)&gt;=0.5</p>
<p>y=0, if  h(x)&lt;0.5</p>
</blockquote>
<p>即malignant=0.5的点投影下来，其右边的点预测y=1;左边预测y=0；则能够很好地进行分类。</p>
<p>那么，如果数据集是这样的呢？</p>
<p><img src="https://i.imgur.com/sYX1vcC.jpg" alt=""></p>
<p>这种情况下，假设linear regression预测为蓝线，那么由0.5的boundary得到的线性方程中，不能很好地进行分类。因为不满足</p>
<blockquote>
<p>y=1, h(x)&gt;0.5</p>
<p>y=0, h(x)&lt;=0.5</p>
</blockquote>
<p>这时，我们引入logistic regression model：</p>
<p><img src="https://i.imgur.com/Wx7geIU.jpg" alt=""></p>
<p>所谓Sigmoid function或Logistic function就是这样一个函数g(z)见上图所示</p>
<p>当z&gt;=0时，g(z)&gt;=0.5；当z&lt;0时，g(z)&lt;0.5</p>
<p>由下图中公式知，给定了数据x和参数θ，y=0和y=1的概率和=1</p>
<p><img src="https://i.imgur.com/7cBL1PU.jpg" alt=""></p>
<h3 id="（三）、decision-boundary"><a href="#（三）、decision-boundary" class="headerlink" title="（三）、decision boundary"></a>（三）、decision boundary</h3><p>所谓Decision Boundary就是能够将所有数据点进行很好地分类的h(x)边界。</p>
<p>如下图所示，假设形如h(x)=g(θ0+θ1x1+θ2x2)的hypothesis参数θ=[-3,1,1]T, 则有</p>
<blockquote>
<p>predict Y=1, if -3+x1+x2&gt;=0</p>
<p>predict Y=0, if -3+x1+x2&lt;0</p>
</blockquote>
<p>刚好能够将图中所示数据集进行很好地分类</p>
<p><img src="https://i.imgur.com/d4ZXADK.jpg" alt=""></p>
<p>Another Example:</p>
<p><img src="https://i.imgur.com/kDklcux.jpg" alt=""></p>
<p>answer:</p>
<p><img src="https://i.imgur.com/S0Jxkhc.jpg" alt=""></p>
<p>除了线性boundary还有非线性decision boundaries，比如<img src="https://i.imgur.com/caD7I7d.jpg" alt=""></p>
<p>下图中，进行分类的decision boundary就是一个半径为1的圆，如图所示：</p>
<p><img src="https://i.imgur.com/DzlXMCB.jpg" alt=""></p>
<h3 id="（四）-（五）Simplified-cost-function-and-gradient-descent-lt-非常重要-gt"><a href="#（四）-（五）Simplified-cost-function-and-gradient-descent-lt-非常重要-gt" class="headerlink" title="（四）~（五）Simplified cost function and gradient descent&lt;非常重要&gt;"></a>（四）~（五）Simplified cost function and gradient descent&lt;非常重要&gt;</h3><p>该部分讲述简化的logistic regression系统中how to implement gradient descents for logistic regression.</p>
<p>假设我们的数据点中y只会取0和1, 对于一个logistic regression model系统，有<img src="https://i.imgur.com/E8WVeN4.jpg" alt="">，那么cost function定义如下：</p>
<p><img src="https://i.imgur.com/1S9Wz8R.jpg" alt=""></p>
<p>由于y只会取0,1，那么就可以写成</p>
<p><img src="https://i.imgur.com/bcN7V2k.jpg" alt=""></p>
<p>不信的话可以把y=0,y=1分别代入，可以发现这个J（θ）和上面的Cost(hθ(x),y)是一样的(<em>^__^</em>) ，那么剩下的工作就是求能最小化 J(θ)的θ了~</p>
<p><img src="https://i.imgur.com/hL9bHvW.jpg" alt=""></p>
<p>在第一章中我们已经讲了如何应用Gradient Descent, 也就是下图Repeat中的部分，将θ中所有维同时进行更新，而J(θ)的导数可以由下面的式子求得，结果如下图手写所示：</p>
<p><img src="https://i.imgur.com/u72ZdGi.jpg" alt=""></p>
<p>现在将其带入Repeat中：</p>
<p><img src="https://i.imgur.com/P0s7nVe.jpg" alt=""></p>
<p>这是我们惊奇的发现，它和第一章中我们得到的公式<img src="https://i.imgur.com/8nLjiKy.jpg" alt="">是一样滴~</p>
<p>也就是说，下图中所示，不管h(x)的表达式是线性的还是logistic regression model, 都能得到如下的参数更新过程。</p>
<p><img src="https://i.imgur.com/hMxFE9x.jpg" alt=""></p>
<p>那么如何用vectorization来做呢？换言之，我们不要用for循环一个个更新θj，而用一个矩阵乘法同时更新整个θ。也就是解决下面这个问题：</p>
<p><img src="https://i.imgur.com/M07DvTL.jpg" alt=""></p>
<p>上面的公式给出了参数矩阵θ的更新，那么下面再问个问题，第二讲中说了如何判断学习率α大小是否合适，那么在logistic regression系统中怎么评判呢？</p>
<p>Q：Suppose you are running gradient descent to fit a logistic regression model with parameter .</p>
<p> Which of the following is a reasonable way to make sure the learning rate  is set properly and that gradient descent is running correctly?</p>
<p>A：<img src="https://i.imgur.com/qVQIWQQ.jpg" alt=""></p>
<h3 id="（六）、Parameter-Optimization-in-Matlab"><a href="#（六）、Parameter-Optimization-in-Matlab" class="headerlink" title="（六）、Parameter Optimization in Matlab"></a>（六）、Parameter Optimization in Matlab</h3><p>这部分内容将对logistic regression 做一些优化措施，使得能够更快地进行参数梯度下降。本段实现了matlab下用梯度方法计算最优参数的过程。</p>
<p>首先声明，除了gradient descent 方法之外，我们还有很多方法可以使用，如下图所示，左边是另外三种方法，右边是这三种方法共同的优缺点，无需选择学习率α，更快，但是更复杂。</p>
<p><img src="https://i.imgur.com/ZI3cECx.jpg" alt=""></p>
<p>也就是matlab中已经帮我们实现好了一些优化参数θ的方法，那么这里我们需要完成的事情只是写好cost function,并告诉系统，要用哪个方法进行最优化参数。比如我们用‘GradObj’， Use the GradObj option to specify that FUN also returns a second output argument G that is the partial derivatives of the function df/dX, at the point X.</p>
<p><img src="https://i.imgur.com/V4JB0fa.jpg" alt=""></p>
<p>如上图所示，给定了参数θ，我们需要给出cost Function. 其中，</p>
<p>jVal 是 cost function 的表示，比如设有两个点（1,0,5）和（0,1,5）进行回归，那么就设方程为hθ(x)=θ1x1+θ2x2;</p>
<p>则有costfunction J(θ)： jVal=(theta(1)-5)^2+(theta(2)-5)^2;</p>
<p>在每次迭代中，按照gradient descent的方法更新参数θ：θ(i)-=gradient(i),其中gradient(i)是J(θ)对θi求导的函数式，</p>
<p>在此例中就有gradient(1)=2<em>(theta(1)-5), gradient(2)=2</em>(theta(2)-5)。如下面代码所示：</p>
<p>函数costFunction, 定义jVal=J(θ)和对两个θ的gradient：</p>
<pre><code>function [ jVal,gradient ] = costFunction( theta )
%COSTFUNCTION Summary of this function goes here
%   Detailed explanation goes here

jVal= (theta(1)-5)^2+(theta(2)-5)^2;

gradient = zeros(2,1);
%code to compute derivative to theta
gradient(1) = 2 * (theta(1)-5);
gradient(2) = 2 * (theta(2)-5);

end
</code></pre><p>编写函数Gradient_descent，进行参数优化</p>
<pre><code>function [optTheta,functionVal,exitFlag]=Gradient_descent( )
%GRADIENT_DESCENT Summary of this function goes here
%   Detailed explanation goes here

 options = optimset(&#39;GradObj&#39;,&#39;on&#39;,&#39;MaxIter&#39;,100);
 initialTheta = zeros(2,1)
 [optTheta,functionVal,exitFlag] = fminunc(@costFunction,initialTheta,options);

end
</code></pre><p>matlab主窗口中调用，得到优化厚的参数(θ1,θ2)=(5,5),即hθ(x)=θ1x1+θ2x2=5<em>x1+5</em>x2</p>
<pre><code> [optTheta,functionVal,exitFlag] = Gradient_descent()

initialTheta =

 0
 0


Local minimum found.

Optimization completed because the size of the gradient is less than
the default value of the function tolerance.

&lt;stopping criteria details&gt;


optTheta =

 5
 5


functionVal =

 0


exitFlag =

 1
</code></pre><p>最后得到的结果显示出优化参数optTheta=[5,5], functionVal = costFunction(迭代后) = 0</p>
<h3 id="（七）、Multi-class-Classification-One-vs-all"><a href="#（七）、Multi-class-Classification-One-vs-all" class="headerlink" title="（七）、Multi-class Classification One-vs-all"></a>（七）、Multi-class Classification One-vs-all</h3><p>所谓one-vs-all method就是将binary分类的方法应用到多类分类中。</p>
<p>比如我想分成K类，那么就将其中一类作为positive，另（k-1）合起来作为negative，这样进行K个h(θ)的参数优化，</p>
<p>每次得到的一个hθ(x)是指给定θ和x，它属于positive的类的概率。</p>
<p><img src="https://i.imgur.com/ZmvupaZ.jpg" alt=""></p>
<p>按照上面这种方法，给定一个输入向量x，获得最大hθ(x)的类就是x所分到的类。</p>
<p><img src="https://i.imgur.com/V96HLwi.jpg" alt=""></p>
<h2 id="第二部分：The-problem-of-overfitting-and-how-to-solve-it"><a href="#第二部分：The-problem-of-overfitting-and-how-to-solve-it" class="headerlink" title="第二部分：The problem of overfitting and how to solve it"></a>第二部分：The problem of overfitting and how to solve it</h2><h3 id="（八）、The-problem-of-overfitting"><a href="#（八）、The-problem-of-overfitting" class="headerlink" title="（八）、The problem of overfitting"></a>（八）、The problem of overfitting</h3><p>The Problem of overfitting:</p>
<p>overfitting就是过拟合，如下图中最右边的那幅图。对于以上讲述的两类（logistic regression和linear regression）都有overfitting的问题，下面分别用两幅图进行解释：</p>
<p><strong><linear regression="">:</linear></strong></p>
<p><img src="https://i.imgur.com/5QPluaA.jpg" alt=""></p>
<p><strong><logistic regression="">:</logistic></strong></p>
<p><img src="https://i.imgur.com/4Tro10y.jpg" alt=""></p>
<p>怎样解决过拟合问题呢？两个方法：</p>
<blockquote>
<ol>
<li><p>减少feature个数（人工定义留多少个feature、算法选取这些feature）</p>
</li>
<li><p>规格化（留下所有的feature，但对于部分feature定义其parameter非常小）</p>
</li>
</ol>
</blockquote>
<p>下面我们将对regularization进行详细的讲解。</p>
<p><img src="https://i.imgur.com/Oz9gRxP.jpg" alt=""></p>
<p>对于linear regression model, 我们的问题是最小化</p>
<p><img src="https://i.imgur.com/6NJaMJB.png" alt=""></p>
<p>写作矩阵表示即</p>
<p><img src="https://i.imgur.com/17xR1Op.png" alt=""></p>
<p>i.e. the loss function can be written as</p>
<p><img src="https://i.imgur.com/k9TkXAs.png" alt=""></p>
<p>there we can get:</p>
<p><img src="https://i.imgur.com/yaenjR4.png" alt=""></p>
<p>After regularization, however,we have:</p>
<p><img src="https://i.imgur.com/psP1pWr.png" alt=""></p>
<h3 id="（九）、Cost-Function"><a href="#（九）、Cost-Function" class="headerlink" title="（九）、Cost Function"></a>（九）、Cost Function</h3><p>对于Regularization，方法如下，定义cost function中θ3，θ4的parameter非常大，那么最小化cost function后就有非常小的θ3,θ4了。</p>
<p><img src="https://i.imgur.com/AqqwbAa.jpg" alt=""></p>
<p>写作公式如下，在cost function中加入θ1~θn的惩罚项：</p>
<p><img src="https://i.imgur.com/UU192wG.jpg" alt=""></p>
<p>这里要注意λ的设置，见下面这个题目：</p>
<p>Q: <img src="https://i.imgur.com/93XDPlR.jpg" alt=""></p>
<pre><code>A:λ很大会导致所有θ≈0
</code></pre><p>下面呢，我们分linear regression 和 logistic regression分别进行regularization步骤.</p>
<h3 id="（十）、Regularized-Linear-Regression"><a href="#（十）、Regularized-Linear-Regression" class="headerlink" title="（十）、Regularized Linear Regression"></a>（十）、Regularized Linear Regression</h3><h4 id=""><a href="#" class="headerlink" title=":"></a><linear regression="">:</linear></h4><p>首先看一下，按照上面的cost function的公式，如何应用gradient descent进行参数更新。</p>
<p>对于θ0，没有惩罚项，更新公式跟原来一样</p>
<p>对于其他θj，J(θ)对其求导后还要加上一项(λ/m)*θj，见下图：</p>
<p><img src="https://i.imgur.com/dFUfhZn.jpg" alt=""></p>
<p>如果不使用梯度下降法（gradient descent+regularization），而是用矩阵计算（normal equation）来求θ，也就求使J(θ)min的θ，令J(θ)对θj求导的所有导数等于0，有公式如下：</p>
<p><img src="https://i.imgur.com/ue7o6Mn.jpg" alt=""></p>
<p>而且已经证明，上面公式中括号内的东西是可逆的。</p>
<h3 id="（十一）、Regularized-Logistic-Regression"><a href="#（十一）、Regularized-Logistic-Regression" class="headerlink" title="（十一）、Regularized Logistic Regression"></a>（十一）、Regularized Logistic Regression</h3><h4 id="-1"><a href="#-1" class="headerlink" title=":"></a><logistic regression="">:</logistic></h4><p>前面已经讲过Logisitic Regression的cost function和overfitting的情况，如下图中所示:</p>
<p><img src="https://i.imgur.com/LAsaVT5.jpg" alt=""></p>
<p>和linear regression一样，我们给J(θ)加入关于θ的惩罚项来抑制过拟合：</p>
<p><img src="https://i.imgur.com/GeD87AF.jpg" alt=""></p>
<p>用Gradient Descent的方法，令J(θ)对θj求导都等于0，得到</p>
<p><img src="https://i.imgur.com/YmYDTlL.jpg" alt=""></p>
<p>这里我们发现，其实和线性回归的θ更新方法是一样的。</p>
<p>When using regularized logistic regression, which of these is the best way to monitor whether gradient descent is working correctly?</p>
<p><img src="https://i.imgur.com/HCgI2h3.jpg" alt=""></p>
<p>和上面matlab中调用那个例子相似，我们可以定义logistic regression的cost function如下所示：</p>
<p><img src="https://i.imgur.com/PzxYfqD.jpg" alt=""></p>
<p>图中，jval表示cost function 表达式，其中最后一项是参数θ的惩罚项；下面是对各θj求导的梯度，其中θ0没有在惩罚项中，因此gradient不变，θ1~θn分别多了一项(λ/m)*θj；</p>
<p>至此，regularization可以解决linear和logistic的overfitting regression问题了~</p>

					
				</div>
			</div>
		</article>

		<ul class="am-pagination">
    
    	<li class="am-pagination-prev">
   		<a class="pull-left" href="/2019/04/16/机器学习实战——梯度下降求解逻辑回归（理论基础）/" title="机器学习实战——梯度下降求解逻辑回归（理论基础）">
      		&laquo; 上一篇
		</a>
		</li>
	
	
		<li class="am-pagination-next">
		<a class="pull-right" href="/2019/04/13/跨站点WebSocket劫持如何导致完整的会话妥协/" title="跨站点WebSocket劫持如何导致完整的会话妥协">
			下一篇 &raquo;
		</a>
		</li>
	 
 </ul>
        

		<div class="theme-annie-comment-button-container">
	<button id="annie-comment-button" class="theme-annie-comment-button" onclick="Annie_Comment()">
		加载评论
		<!--加载评论-->
	</button>
</div>

<div id="annie-comment-container" class="theme-annie-comment-main-container">

	
		
			<!-- comment gitalk -->
			<!-- show gitalk comment -->

  <div id="gitalk-container"></div>


<!-- gitalk`s css & js -->
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<link rel="stylesheet" href="/css/comment.css">
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<script type="text/javascript">
	//thanks O-R
	//https://github.com/gitalk/gitalk/issues/102#issuecomment-382970552
	//去除尾部匹配正则数组的字符串  
	//remove redundant characters
	String.prototype.trimEnd = function(regStr) {
		var result = this;
		if(regStr == undefined || regStr == null || regStr == "") {
			return result;
		}
		var array = regStr.split(',');

		if(array.length > 0) {

			var c = array.shift();
			var str = this;
			var i = str.length;
			var rg = new RegExp(c);
			var matchArr = str.match(rg);

			if(matchArr != undefined && matchArr != null && matchArr.length > 0) {
				var matchStr = matchArr[0].replace(/\\/g, "\\\\").replace(/\*/g, "\\*")
					.replace(/\+/g, "\\+").replace(/\|/g, "\\|")
					.replace(/\{/g, "\\{").replace(/\}/g, "\\}")
					.replace(/\(/g, "\\(").replace(/\)/g, "\\)")
					.replace(/\^/g, "\\^").replace(/\$/g, "\\$")
					.replace(/\[/g, "\\[").replace(/\]/g, "\\]")
					.replace(/\?/g, "\\?").replace(/\,/g, "\\,")
					.replace(/\./g, "\\.").replace(/\&/g, "\\&");
				matchStr = matchStr + '$';
				result = str.replace(new RegExp(matchStr), "");
			}

			if(array.length > 0) {
				return result.trimEnd(array.join())
			} else {
				return result;
			}
		}
	};

	//create gitalk
	var gitalk = new Gitalk({
		clientID: '74a83cc9b15a1cf53e10',
		clientSecret: 'b62af8b132c33178adcb0098e2315806e2e57cae',
		//id: window.location.pathname,
		// id: (window.location.pathname).split("/").pop().substring(0, 49),
		id: md5(location.href.trimEnd('#.*$,\\?.*$,index.html$')),
		repo: 'Annie-Gitalk',
		owner: 'miraitowa',
		admin: 'miraitowa',
		distractionFreeMode: 'true',
	})
	gitalk.render('gitalk-container');
</script>
		
	

</div>

<script type="text/javascript">
	/* Show Comment */
	var Annie_Comment = function() {
		function Show_Hidden(obj) {
			obj.style.display = 'block';
		}
		
		//var obutton = $('#annie-comment-button');
		//var obutton = $('#annie-comment-container');
		var obutton = document.getElementById("annie-comment-button" || "0");
		var odiv = document.getElementById("annie-comment-container");
		if( 'obutton' ) {
			obutton.onclick = function() {
				Show_Hidden(odiv);
				$("#annie-comment-button").css("display", 'none');
				return false;
			}
		}
	};

	(function Annie_Init() {
		Annie_Comment();
	})();
</script>
		
		<!--
	时间：2018-09-24
	描述：The TOC module refers to 'https://github.com/codefine/hexo-theme-mellow', include toc.ejs、toc.js、toc.css. All rights reserved by codefine. 
-->

	
		<aside class="post-widget">
			<nav class="post-toc-wrap" id="post-toc">
				
					<strong>文章目录</strong>
				
				
				<!--toc(post.content)-->
				<ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Stanford机器学习-逻辑回归和过拟合问题的解决-logistic-Regression-amp-Regularization"><span class="post-toc-text">Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression &amp; Regularization</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第一部分：Logistic-Regression"><span class="post-toc-text">第一部分：Logistic Regression</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（一）-（二）、Classification-Hypothesis-Representation"><span class="post-toc-text">（一）~（二）、Classification / Hypothesis Representation</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（三）、decision-boundary"><span class="post-toc-text">（三）、decision boundary</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（四）-（五）Simplified-cost-function-and-gradient-descent-lt-非常重要-gt"><span class="post-toc-text">（四）~（五）Simplified cost function and gradient descent&lt;非常重要&gt;</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（六）、Parameter-Optimization-in-Matlab"><span class="post-toc-text">（六）、Parameter Optimization in Matlab</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（七）、Multi-class-Classification-One-vs-all"><span class="post-toc-text">（七）、Multi-class Classification One-vs-all</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#第二部分：The-problem-of-overfitting-and-how-to-solve-it"><span class="post-toc-text">第二部分：The problem of overfitting and how to solve it</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（八）、The-problem-of-overfitting"><span class="post-toc-text">（八）、The problem of overfitting</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（九）、Cost-Function"><span class="post-toc-text">（九）、Cost Function</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（十）、Regularized-Linear-Regression"><span class="post-toc-text">（十）、Regularized Linear Regression</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#"><span class="post-toc-text">:</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#（十一）、Regularized-Logistic-Regression"><span class="post-toc-text">（十一）、Regularized Logistic Regression</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#-1"><span class="post-toc-text">:</span></a></li></ol></li></ol></li></ol></li></ol>
			</nav>
			<div class="post-toc-bar"><div>
		</div></div></aside>
	

	</div>
</div>
		</div>
		</main>
		
		<!--footer-->
		<footer>
	<div class="blog-text-center">
		<div class="theme-annie-social">
				
				
					<a href="https://github.com/miraitowa" title="Github" target="_blank"><i class="fa fa-github"></i>&nbsp;</a>
					
				
				
					<a href="https://miraitowa.github.io/about/" title="Email" target="_blank"><i class="fa fa-envelope-o"></i>&nbsp;</a>
					
					
						
				
		</div>
	</div>

	<div class="blog-text-center">
		<div class="theme-annie-copyright">
			
				&copy; 2018 11 11 - 2019, content by miraitowa. All Rights Reserved.			       	
			
		</div>
	</div>

	<div class="blog-text-center">
		<div class="theme-annie-copyright">
			<a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> Theme <a href="https://github.com/Sariay/hexo-theme-Annie" title="Annie" target="_blank" rel="noopener">Annie</a> by Sariay.		
		</div>
	</div>
</footer>
		<!-- <script src="http://code.jquery.com/jquery-2.1.1.min.js" type="text/javascript"></script> -->

<script>
	window.jQuery || document.write('<script src="/js/jquery-2.1.1.min.js"><\/script>')
</script>

<style>
	.motto {
		color: #000000;
		font-size: 20px;
		margin: 100px 25% 0;
		width: 50%;
		line-height: 1.4;
		font-family:"KaiTi", "STXingkai", "Source Sans Pro", "Segoe UI", "Lucida Grande", Helvetica, Arial, "Microsoft YaHei", FreeSans, Arimo, "Droid Sans", "wenquanyi micro hei", "Hiragino Sans GB", "Hiragino Sans GB W3", FontAwesome, sans-serif;
		text-align: center;
	}
	@media(max-width: 890px) {
		.motto {	
			margin: 100px 10% 0;
			width: 80%;
		}
	}
	@media(max-width: 890px) {
		.motto {
			margin: 100px 5% 0;
			width: 90%;
		}
	}
</style>


	<script src="/js/motto.js"></script>
	<script type="text/javascript">
		$(".motto").html(getMingYanContent());
	</script>	







	<script type="text/javascript" src="/js/toc.js"></script>


<script type="text/javascript" src="/js/main.js"></script>

<script type="text/javascript">
	//generate a random img that pre_name 'from 0 to 110'
	var random_bg = Math.floor(Math.random() * 109 + 1);

	//var bg = 'url(/img/random/' + random_bg + '.jpg)';		
	var bg = 'url(https://miraitowa.github.io/Random-img/' + random_bg + '.jpg)';

	$("#header-bg-2").css("background-image", bg);
</script>
		
		<!--back to top-->
        <style type="text/css">
	#totop {
		background: white;
		border-radius: 50%;
		position: fixed;
		right: 5.4%;
		bottom: 80px;
		cursor: pointer;
	}
	
	#totop a {
		color: #474747;
		background-color: transparent;
		padding: 10px;
		text-decoration: none;
	}
	
	@media(max-width:512px) {
		#totop {
			display: none;
			visibility: hidden;
		}
	}
</style>


	<div id="totop">
  		<a href="javascript:;" class="fa fa-arrow-up"></a>
	</div>

	</body>
	</html>

