<!--
	作者：Sariay
	时间：2018-09-25
	描述：There may be a bug, but don't worry, QiLing(器灵) says that it can work normally!
-->


	<!DOCTYPE html>
	<html>
		

<head><meta name="generator" content="Hexo 3.8.0">
	<title>机器学习实战——梯度下降求解逻辑回归（理论基础）</title>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="apple-mobile-web-app-title" content="Amaze UI">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
    <meta name="author" content="miraitowa">
    <meta name="keywords" content="">
    <meta name="description" content="">
   	<!-- css -->
	<link rel="stylesheet" href="/css/style.css">

	<!-- favicon -->
	<link href="/img/favicon.ico" rel="Shortcut Icon" type="image/ico">
	
	<!-- font-awesome -->
	<link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
</head>

	<body>	
		<!--Preloader-->
<div id="preloader">
	<div id="status">
		<img alt="PRELOADER" src="/img/logo.png">
	</div>
</div>
<!--Preloader end-->

<!-- header -->

	<header id="header-bg-2">

	
		<div id="cd-logo"><a href="/"><img src="/img/logo.png" alt="Logo"></a></div>
	
	
	<!-- motto or description -->
		
 		<p class="motto"></p>
	
	
	<!-- current page name or title -->
	
		
		
			
			<p class="page-name">当前文章&nbsp;:&nbsp;《机器学习实战——梯度下降求解逻辑回归（理论基础）》</p>
			
		
	
	
	<!-- others: such as change-bg, time... -->
	<p class="page-name-other">
		11/28/2019 
		<style type="text/css">
	header:after {
		content: '';
		position: relative;
		top: 0;
		left: 0;
		height: 100%;
		width: 100%;
		background: #222222;
		opacity: .5;
		z-index: -1;
	}
	
	.change-header-bg{
		font-style: normal;
	}
	.change-header-bg i{
		text-align: center;
		cursor: pointer;
		pointer-events: bounding-box;
	}
	@media(max-width:512px) {
		.change-header-bg {
			display: none;
			visibility: hidden;
		}
	}
	
</style>

<script type="text/javascript">
	function changeHeaderBg(){
		var random_bg = Math.floor(Math.random() * 109 + 1);
		var bg = 'url(https://miraitowa.github.io/Random-img/' + random_bg + '.jpg)';
		$("#header-bg-2").css("background-image", bg);
	}
</script>

<span class="change-header-bg">
	——&nbsp;<i class="fa fa-camera-retro" onclick="changeHeaderBg()"></i>	
</span>
	</p>		
</header>

<!-- nav -->
<div id="cd-nav">
	<a href="#0" title="menu" class="cd-nav-trigger"><span></span></a>

	<nav id="cd-main-nav">
		<ul>
			
      		<li class="fa fa-/">
           		<a href="/" title="主页">主页</a>	
      		</li>
    		
      		<li class="fa fa-/archives">
           		<a href="/archives" title="归档">归档</a>	
      		</li>
    		
      		<li class="fa fa-/categories">
           		<a href="/categories" title="分类">分类</a>	
      		</li>
    		
      		<li class="fa fa-/gallery">
           		<a href="/gallery" title="相册">相册</a>	
      		</li>
    		
      		<li class="fa fa-/about">
           		<a href="/about" title="关于">关于</a>	
      		</li>
    		
      		<li class="fa fa-/tags">
           		<a href="/tags" title="友链">友链</a>	
      		</li>
    		
    		
        	
		</ul>
	</nav>
</div>

		<!--main-->
		<main> 
		<div class="page-container">
		<!-- content srart -->
<div class="am-g am-g-fixed blog-fixed blog-content">
	<div class="am-u-md-8 am-u-sm-12">

		<article class="am-article blog-article-p">

			<div class="am-article-hd">
				


				<h1 class="am-article-title blog-text-center">
					
					
	
		<a href="/2019/04/16/机器学习实战——梯度下降求解逻辑回归（理论基础）/" itemprop="url">		
			机器学习实战——梯度下降求解逻辑回归（理论基础）		
		</a>
	

				</h1>

				<p class="am-article-meta blog-text-center">
					<span>
						<i class="fa fa-clock-o"></i> 
						<a href="/2019/04/16/机器学习实战——梯度下降求解逻辑回归（理论基础）/" itemprop="url">
	<time datetime="2019-04-16T13:10:16.649Z" itemprop="datePublished">
  		2019-04-16
  </time>
</a>    
&nbsp;
					</span>
					
					<span>						
						
					</span>
				</p>
			</div>

			<div class="am-article-bd">
				<div class="content" id="post-content">
					
						<h2 id="机器学习实战——梯度下降求解逻辑回归（理论基础）"><a href="#机器学习实战——梯度下降求解逻辑回归（理论基础）" class="headerlink" title="机器学习实战——梯度下降求解逻辑回归（理论基础）"></a>机器学习实战——梯度下降求解逻辑回归（理论基础）</h2><p>逻辑回归是回归还是分类？<br>逻辑回归是分类，不要被名字所欺骗。因本篇文章仅讨论二分类问题，故我们将逻辑回归最终得到的预测值看作两个，即是或否（0或1）。<br>从线性回归开始<br>为什么从线性回归开始呢？因为二分类问题解的得出与线性回归有很大关系，逻辑回归之所以叫回归因为其与线性回归有着千丝万缕的关系。<br>有这样一个例子：<br>我们用numpy包生成一百个随机的x值，并且设定y值与x有一定的线性关系，最后把这样的线性关系绘制出来：</p>
<pre><code>import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
%matplotlib inline

# 使用numpy生成100个随机点   样本

x_data = np.random.rand(100)

y_data = x_data * 0.1 + 0.2

fig, ax = plt.subplots()

ax.scatter(x_data, y_data)

ax.set_xlabel(&#39;x&#39;)

ax.set_ylabel(&#39;y&#39;)

plt.show()
</code></pre><p>绘制出来的图是这样的：</p>
<p><img src="https://i.imgur.com/jZMNwvR.png" alt=""></p>
<p>线性回归的目标便是求得一条曲线，能最大程度拟合我们的数据点（X1、X2轴），而曲线的Y值便是我们的预测值。其实就是个立体的曲线，图例如下：</p>
<p><img src="https://i.imgur.com/2GpJHr3.png" alt=""></p>
<p>即线性回归得出的是连续的结果，如果我们仅仅需要得到离散的结果，即是或否，由此便引出了二分类问题，也就是本文要讨论的逻辑回归方法。</p>
<h3 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h3><p>现要实现一个简单的逻辑回归：</p>
<blockquote>
<p>我们将建立一个逻辑回归模型来预测一个学生是否被大学录取。假设你是一个大学系的管理员，你想根据两次考试的结果来决定每个申请人的录取机会。你有以前的申请人的历史数据，你可以用它作为逻辑回归的训练集。对于每一个培训例子，你有两个考试的申请人的分数和录取决定。为了做到这一点，我们将建立一个分类模型，根据考试成绩估计入学概率。</p>
</blockquote>
<p>即要求我们通过一些数据集来训练电脑，能实现输入两门考试成绩从而得到是否录取的结果。</p>
<p>设X1为exam1的成绩，X2为exam2的成绩，X1、X2就是我们的两个特征。</p>
<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

import os
path = &quot;data&quot; + os.sep + &quot;LogiReg_data.txt&quot;
pdData = pd.read_csv(path, header=None, names=[&#39;Exam 1&#39;, &#39;Exam 2&#39;, &#39;Admitted&#39;])
pdData.head()
</code></pre><p><img src="https://i.imgur.com/iPBfbCc.png" alt=""></p>
<p>可以看到学生有两门成绩，学校是通过两门成绩来决定是否录取。</p>
<p>之后我们可以利用python的绘图包通过散点图的绘制来将数据更直接的表现出来。</p>
<pre><code>positive = pdData[pdData[&#39;Admitted&#39;] == 1]
negative = pdData[pdData[&#39;Admitted&#39;] ==0 ]

fig, ax = plt.subplots(figsize=(10,5))
ax.scatter(positive[&#39;Exam 1&#39;], positive[&#39;Exam 2&#39;], s=30, c=&#39;b&#39;, marker=&#39;o&#39;, label=&#39;Admitted&#39;)
ax.scatter(negative[&#39;Exam 1&#39;], negative[&#39;Exam 2&#39;], s=30, c=&#39;r&#39;, marker=&#39;x&#39;, label=&#39;Not Admitted&#39;)
ax.legend()
ax.set_xlabel(&#39;Exam 1 Score&#39;)
ax.set_ylabel(&#39;Exam 2 Score&#39;)
</code></pre><p><img src="https://i.imgur.com/2gQK94d.png" alt=""></p>
<h3 id="初步求解"><a href="#初步求解" class="headerlink" title="初步求解"></a>初步求解</h3><p>我们求解的线性关系一定是有参数的，因为我们有两个特征值，而我们需要通过两个特征值求得预测值，但是两个特征值对结果的影响又不尽相同，</p>
<p>所以我们需要两个参数来度量两个特征值对结果影响程度，以及一个参数来充当偏置项（曲线会上下浮动，且偏置项对结果作用较小）：</p>
<p><img src="https://i.imgur.com/TopLoMs.png" alt=""></p>
<p>所以我们的预测结果可以表示为：</p>
<p><img src="https://i.imgur.com/Q0VXVwD.png" alt=""></p>
<p>整合之后：</p>
<p><img src="https://i.imgur.com/mNWx2KL.png" alt=""></p>
<p>这便是我们构造的预测结果值，但是仅仅有结果值还是不够的，我们需要将预测结果值转化为录取的概率，我们规定：当概率大于0.5则Y可以取1表示录用，小于0.5不录用Y取0，所以我们又需要一个函数来转化预测值为概率，称为sigmoid函数，定义如下：</p>
<p><img src="https://i.imgur.com/WPMIWOE.png" alt=""></p>
<pre><code>def sigmoid(z):
    return 1 / (1 + np.exp(-z))

nums = np.arange(-10, 10 ,step=1)
fig, ax = plt.subplots(figsize=(12,4))
ax.plot(nums, sigmoid(nums), &#39;r&#39;)
</code></pre><p><img src="https://i.imgur.com/tnFbTq2.png" alt=""></p>
<p>可以看到该函数符合我们的需求，其取值位于0到1，定义域为实数集R，可以将我们的预测结果转化为概率！</p>
<p>而所谓的逻辑回归便是将任意的输入值映射到[0,1]区间上，将我们通过线性回归得到的预测值转化为概率，完成分类任务。</p>
<h3 id="初步求解-1"><a href="#初步求解-1" class="headerlink" title="初步求解"></a>初步求解</h3><p>我们载入数据，大体明白了预测结果长什么样，以及值概率的转化，但我们的模型还完全没有建立起来！</p>
<p>我们知道所谓机器学习便是我们交给机器一堆数据，然后告诉它什么样的学习方式是对的（目标函数），叫它朝着这个方向走，然后还要规定每次走的步长（学习率）</p>
<p><img src="https://i.imgur.com/QKFAyEu.png" alt=""></p>
<p>如同这样一个山谷，我们要达到山谷的最低点，利用梯度下降的方法，每经过一个数据点，便运行更新函数更新下一步的方向（梯度，求偏导）。<br>所以我们还要做损失函数（目标函数）、更新函数。</p>
<h3 id="何为损失函数？"><a href="#何为损失函数？" class="headerlink" title="何为损失函数？"></a>何为损失函数？</h3><p>我们通过X来估计Y的值，预测值可能符合真实值，也可能不符合真实值，所以我们引入损失函数来度量拟合的程度，损失函数越小代表拟合的越好。<br>在此处我们暂时将损失函数视为目标函数。</p>
<h3 id="关于梯度下降"><a href="#关于梯度下降" class="headerlink" title="关于梯度下降"></a>关于梯度下降</h3><p>梯度下降便是在凸函数中沿着梯度下降的方向不断更新参数，一般情况下我们通过加负号实现。</p>
<p>梯度下降有三种方式实现：</p>
<p><img src="https://i.imgur.com/PpOxKGb.png" alt=""></p>
<h4 id="1-批量梯度下降法（Batch-Gradient-Descent）"><a href="#1-批量梯度下降法（Batch-Gradient-Descent）" class="headerlink" title="1 批量梯度下降法（Batch Gradient Descent）"></a>1 批量梯度下降法（Batch Gradient Descent）</h4><p>批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新</p>
<h4 id="2-随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#2-随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="2 随机梯度下降法（Stochastic Gradient Descent）"></a>2 随机梯度下降法（Stochastic Gradient Descent）</h4><p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。</p>
<p>随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。</p>
<p>对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，</p>
<p>训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，</p>
<p>由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是小批量梯度下降法。</p>
<h4 id="3-小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#3-小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="3 小批量梯度下降法（Mini-batch Gradient Descent）"></a>3 小批量梯度下降法（Mini-batch Gradient Descent）</h4><p>小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。</p>
<h3 id="预测函数（完成值到概率的转化）："><a href="#预测函数（完成值到概率的转化）：" class="headerlink" title="预测函数（完成值到概率的转化）："></a>预测函数（完成值到概率的转化）：</h3><p><img src="https://i.imgur.com/YCuteoV.png" alt=""></p>
<h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><p>常说的概率是指给定参数后，预测即将发生的事件的可能性。而似然概率正好与这个过程相反，我们关注的量不再是事件的发生概率，</p>
<p>而是已知发生了某些事件，我们希望知道参数应该是多少。我们的似然函数定义如下：</p>
<p><img src="https://i.imgur.com/AlWpW7d.png" alt=""></p>
<p>即表示我们预测的参数满足所有样本值这一事件的概率，接下来要做的就是极大似然估计，即令参数的取值无限拟合我们的真实数据。</p>
<p>我们取对数似然，此时应用梯度上升求最大值，再引入目标函数转换为梯度下降求最小值(加负号，除以样本总数，考虑整体样本)，求偏导，令其等于零。目标函数如下：</p>
<p><img src="https://i.imgur.com/X3op89p.png" alt=""></p>
<p>求偏导的过程：</p>
<p><img src="https://i.imgur.com/t8k9JJC.png" alt=""></p>

					
				</div>
			</div>
		</article>

		<ul class="am-pagination">
    
    	<li class="am-pagination-prev">
   		<a class="pull-left" href="/2019/04/25/破解实验室WiFi密码/" title="破解实验室WiFi密码">
      		&laquo; 上一篇
		</a>
		</li>
	
	
		<li class="am-pagination-next">
		<a class="pull-right" href="/2019/04/14/Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression & Regularization/" title="Stanford机器学习 逻辑回归和过拟合问题的解决 logistic Regression & Regularization">
			下一篇 &raquo;
		</a>
		</li>
	 
 </ul>
        

		<div class="theme-annie-comment-button-container">
	<button id="annie-comment-button" class="theme-annie-comment-button" onclick="Annie_Comment()">
		加载评论
		<!--加载评论-->
	</button>
</div>

<div id="annie-comment-container" class="theme-annie-comment-main-container">

	
		
			<!-- comment gitalk -->
			<!-- show gitalk comment -->

  <div id="gitalk-container"></div>


<!-- gitalk`s css & js -->
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<link rel="stylesheet" href="/css/comment.css">
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<script type="text/javascript">
	//thanks O-R
	//https://github.com/gitalk/gitalk/issues/102#issuecomment-382970552
	//去除尾部匹配正则数组的字符串  
	//remove redundant characters
	String.prototype.trimEnd = function(regStr) {
		var result = this;
		if(regStr == undefined || regStr == null || regStr == "") {
			return result;
		}
		var array = regStr.split(',');

		if(array.length > 0) {

			var c = array.shift();
			var str = this;
			var i = str.length;
			var rg = new RegExp(c);
			var matchArr = str.match(rg);

			if(matchArr != undefined && matchArr != null && matchArr.length > 0) {
				var matchStr = matchArr[0].replace(/\\/g, "\\\\").replace(/\*/g, "\\*")
					.replace(/\+/g, "\\+").replace(/\|/g, "\\|")
					.replace(/\{/g, "\\{").replace(/\}/g, "\\}")
					.replace(/\(/g, "\\(").replace(/\)/g, "\\)")
					.replace(/\^/g, "\\^").replace(/\$/g, "\\$")
					.replace(/\[/g, "\\[").replace(/\]/g, "\\]")
					.replace(/\?/g, "\\?").replace(/\,/g, "\\,")
					.replace(/\./g, "\\.").replace(/\&/g, "\\&");
				matchStr = matchStr + '$';
				result = str.replace(new RegExp(matchStr), "");
			}

			if(array.length > 0) {
				return result.trimEnd(array.join())
			} else {
				return result;
			}
		}
	};

	//create gitalk
	var gitalk = new Gitalk({
		clientID: '74a83cc9b15a1cf53e10',
		clientSecret: 'b62af8b132c33178adcb0098e2315806e2e57cae',
		//id: window.location.pathname,
		// id: (window.location.pathname).split("/").pop().substring(0, 49),
		id: md5(location.href.trimEnd('#.*$,\\?.*$,index.html$')),
		repo: 'Annie-Gitalk',
		owner: 'miraitowa',
		admin: 'miraitowa',
		distractionFreeMode: 'true',
	})
	gitalk.render('gitalk-container');
</script>
		
	

</div>

<script type="text/javascript">
	/* Show Comment */
	var Annie_Comment = function() {
		function Show_Hidden(obj) {
			obj.style.display = 'block';
		}
		
		//var obutton = $('#annie-comment-button');
		//var obutton = $('#annie-comment-container');
		var obutton = document.getElementById("annie-comment-button" || "0");
		var odiv = document.getElementById("annie-comment-container");
		if( 'obutton' ) {
			obutton.onclick = function() {
				Show_Hidden(odiv);
				$("#annie-comment-button").css("display", 'none');
				return false;
			}
		}
	};

	(function Annie_Init() {
		Annie_Comment();
	})();
</script>
		
		<!--
	时间：2018-09-24
	描述：The TOC module refers to 'https://github.com/codefine/hexo-theme-mellow', include toc.ejs、toc.js、toc.css. All rights reserved by codefine. 
-->

	
		<aside class="post-widget">
			<nav class="post-toc-wrap" id="post-toc">
				
					<strong>文章目录</strong>
				
				
				<!--toc(post.content)-->
				<ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#机器学习实战——梯度下降求解逻辑回归（理论基础）"><span class="post-toc-text">机器学习实战——梯度下降求解逻辑回归（理论基础）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#问题的提出"><span class="post-toc-text">问题的提出</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#初步求解"><span class="post-toc-text">初步求解</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#初步求解-1"><span class="post-toc-text">初步求解</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#何为损失函数？"><span class="post-toc-text">何为损失函数？</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#关于梯度下降"><span class="post-toc-text">关于梯度下降</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#1-批量梯度下降法（Batch-Gradient-Descent）"><span class="post-toc-text">1 批量梯度下降法（Batch Gradient Descent）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#2-随机梯度下降法（Stochastic-Gradient-Descent）"><span class="post-toc-text">2 随机梯度下降法（Stochastic Gradient Descent）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#3-小批量梯度下降法（Mini-batch-Gradient-Descent）"><span class="post-toc-text">3 小批量梯度下降法（Mini-batch Gradient Descent）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#预测函数（完成值到概率的转化）："><span class="post-toc-text">预测函数（完成值到概率的转化）：</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#似然函数"><span class="post-toc-text">似然函数</span></a></li></ol></li></ol>
			</nav>
			<div class="post-toc-bar"><div>
		</div></div></aside>
	

	</div>
</div>
		</div>
		</main>
		
		<!--footer-->
		<footer>
	<div class="blog-text-center">
		<div class="theme-annie-social">
				
				
					<a href="https://github.com/miraitowa" title="Github" target="_blank"><i class="fa fa-github"></i>&nbsp;</a>
					
				
				
					<a href="https://miraitowa.github.io/about/" title="Email" target="_blank"><i class="fa fa-envelope-o"></i>&nbsp;</a>
					
					
						
				
		</div>
	</div>

	<div class="blog-text-center">
		<div class="theme-annie-copyright">
			
				&copy; 2018 11 11 - 2019, content by miraitowa. All Rights Reserved.			       	
			
		</div>
	</div>

	<div class="blog-text-center">
		<div class="theme-annie-copyright">
			<a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> Theme <a href="https://github.com/Sariay/hexo-theme-Annie" title="Annie" target="_blank" rel="noopener">Annie</a> by Sariay.		
		</div>
	</div>
</footer>
		<!-- <script src="http://code.jquery.com/jquery-2.1.1.min.js" type="text/javascript"></script> -->

<script>
	window.jQuery || document.write('<script src="/js/jquery-2.1.1.min.js"><\/script>')
</script>

<style>
	.motto {
		color: #000000;
		font-size: 20px;
		margin: 100px 25% 0;
		width: 50%;
		line-height: 1.4;
		font-family:"KaiTi", "STXingkai", "Source Sans Pro", "Segoe UI", "Lucida Grande", Helvetica, Arial, "Microsoft YaHei", FreeSans, Arimo, "Droid Sans", "wenquanyi micro hei", "Hiragino Sans GB", "Hiragino Sans GB W3", FontAwesome, sans-serif;
		text-align: center;
	}
	@media(max-width: 890px) {
		.motto {	
			margin: 100px 10% 0;
			width: 80%;
		}
	}
	@media(max-width: 890px) {
		.motto {
			margin: 100px 5% 0;
			width: 90%;
		}
	}
</style>


	<script src="/js/motto.js"></script>
	<script type="text/javascript">
		$(".motto").html(getMingYanContent());
	</script>	







	<script type="text/javascript" src="/js/toc.js"></script>


<script type="text/javascript" src="/js/main.js"></script>

<script type="text/javascript">
	//generate a random img that pre_name 'from 0 to 110'
	var random_bg = Math.floor(Math.random() * 109 + 1);

	//var bg = 'url(/img/random/' + random_bg + '.jpg)';		
	var bg = 'url(https://miraitowa.github.io/Random-img/' + random_bg + '.jpg)';

	$("#header-bg-2").css("background-image", bg);
</script>
		
		<!--back to top-->
        <style type="text/css">
	#totop {
		background: white;
		border-radius: 50%;
		position: fixed;
		right: 5.4%;
		bottom: 80px;
		cursor: pointer;
	}
	
	#totop a {
		color: #474747;
		background-color: transparent;
		padding: 10px;
		text-decoration: none;
	}
	
	@media(max-width:512px) {
		#totop {
			display: none;
			visibility: hidden;
		}
	}
</style>


	<div id="totop">
  		<a href="javascript:;" class="fa fa-arrow-up"></a>
	</div>

	</body>
	</html>

